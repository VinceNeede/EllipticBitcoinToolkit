{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69f6e82",
   "metadata": {},
   "source": [
    "# Elliptic Bitcoin Dataset: Random Forest Example\n",
    "\n",
    "This notebook demonstrates how to load, preprocess, and model the Elliptic Bitcoin dataset using a Random Forest classifier. It covers:\n",
    "\n",
    "- Downloading and preparing the dataset\n",
    "- Training a machine learning model with a scikit-learn pipeline\n",
    "- Performing hyperparameter tuning with temporal cross-validation\n",
    "- Visualizing results and evaluating model performance over time\n",
    "\n",
    "This example is intended for both documentation and as a reproducible reference for users of the `elliptic_toolkit` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c27ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elliptic_toolkit.dataset import download_dataset, load_labeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1eca0",
   "metadata": {},
   "source": [
    "# Loading the Elliptic Bitcoin Dataset\n",
    "\n",
    "To work with the Elliptic Bitcoin dataset, you first need to ensure the data is available locally. Use the `download_dataset` function to automatically download the dataset from PyTorch Geometric. The data will be saved in the `elliptic_bitcoin_dataset` folder by default, which will be created if it does not already exist.\n",
    "\n",
    "- If the dataset files are already present, they will not be downloaded again unless you set `force=True`.\n",
    "- This process ensures you always have the required data in the correct location for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87316e9e",
   "metadata": {},
   "source": [
    "Now that the dataset is available, you can load it into memory using the `load_labeled_data` utility function. This function:\n",
    "\n",
    "- Maps the class labels as follows:\n",
    "  - `1`: Illicit\n",
    "  - `0`: Licit\n",
    "  - `-1`: Unknown\n",
    "- Maps transaction indices to row indices for easier data handling.\n",
    "- Automatically performs a temporal train/test split, where by default the latest 20% of time steps are reserved for testing.\n",
    "\n",
    "This setup ensures your data is ready for machine learning workflows, with clear class labels and a reproducible split between training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d2768",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_labeled_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c6e94",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "\n",
    "With the data prepared, you can now train a machine learning model using your preferred scikit-learn estimator. In this example, we use a pipeline that includes the custom `DropTime` transformer, which removes the time column from the features. This step is important to prevent the model from learning spurious correlations based on time.\n",
    "\n",
    "- Select any scikit-learn compatible model (here, a random forest is used).\n",
    "- The pipeline ensures preprocessing and modeling steps are applied consistently.\n",
    "\n",
    "This approach helps maintain the integrity of your evaluation by avoiding data leakage from temporal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b31c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "from elliptic_toolkit.model_wrappers import DropTime\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"drop_time\", DropTime()),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=10, random_state=42))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "PrecisionRecallDisplay.from_estimator(pipe, X_test, y_test, name=\"Random Forest\")\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa4de79",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "To further improve model performance, you can perform hyperparameter tuning using cross-validation. The `TemporalRollingCV` cross-validator is designed for time-dependent data: it creates temporally ordered splits, similar to `sklearn.TimeSeriesSplit`, but ensures that training and test sets do not have overlapping time indices (important since time is an aggregated feature in this dataset).\n",
    "\n",
    "This approach helps prevent data leakage and provides a more realistic evaluation of model performance on future, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48002f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from elliptic_toolkit.temporal_cv import TemporalRollingCV\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid={\n",
    "        \"clf__n_estimators\": [10, 50, 100],\n",
    "    },\n",
    "    cv=TemporalRollingCV(n_splits=3),\n",
    "    scoring=\"average_precision\",\n",
    "    n_jobs=1,\n",
    "    verbose=10\n",
    ")\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a7f15",
   "metadata": {},
   "source": [
    "## Visualizing hyperparameter search results\n",
    "\n",
    "To better understand the impact of each hyperparameter on model performance, you can visualize the marginal effects using the `plot_marginals` utility function. These plots show how changes in a single hyperparameter affect the evaluation score, helping you identify which parameters are most influential and guiding further tuning decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elliptic_toolkit.plots import plot_marginals\n",
    "\n",
    "for marginal in plot_marginals(grid.cv_results_):\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c41849",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "To thoroughly evaluate your model, you can use the `plot_evals` utility function. This function provides both a Precision-Recall curve and a rolling evaluation plot, allowing you to:\n",
    "\n",
    "- Assess the overall precision and recall of your model.\n",
    "- Visualize how model performance changes as you test on data further away in time from the training period.\n",
    "- Reference the illicit rate, which is plotted on a separate axis, to help contextualize model performance relative to the prevalence of illicit transactions over time.\n",
    "\n",
    "This temporal evaluation is especially useful for understanding how well your model generalizes to future, unseen data and for detecting any performance degradation over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elliptic_toolkit.plots import plot_evals\n",
    "\n",
    "for fig in plot_evals(grid, X_test, y_test, y_train):\n",
    "    plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313_torch_cuda216",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
